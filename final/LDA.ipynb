{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The first thing to do is to import the relevant packages\n",
    "# that I will need for my script, \n",
    "# these include the Numpy (for maths and arrays)\n",
    "# and csv for reading and writing csv files\n",
    "# If i want to use something from this I need to call \n",
    "# csv.[function] or np.[function] first\n",
    "\n",
    "import csv as csv \n",
    "import numpy as np\n",
    "\n",
    "# Open up the csv file in to a Python object\n",
    "data_all = []\n",
    "with open('events_pdloire.csv', encoding=\"utf8\") as train_file:\n",
    "    csv_reader = csv.reader(train_file, delimiter=';', quotechar='\"')\n",
    "    for row in csv_reader:\n",
    "        data_all.append(row)\n",
    "data_all = np.array(data_all)\n",
    "data = data_all[1::]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiation LDA model \n",
    "Nota Bene : Because this is unsupervised learning, you can't just arbitrarily set a number of topics.  Its similar to K-means clustering, you have to find the ideal number of centroids.  And in similar ways, there are methods to help you do this, but its not perfect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Comment le numérique peut nous aider à fluidif...</td>\n",
       "      <td>Dans un monde où tous les secteurs sont transf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Village des sciences d'Angers</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rencontre autour des nouveaux usages, de la pa...</td>\n",
       "      <td>Que recouvre aujourd’hui la notion de particip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>En 2015, on fêtait la première détection d'une...</td>\n",
       "      <td>En 2015, on fêtait la première détection d'une...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Il sera proposé aux jeunes élèves d'utiliser l...</td>\n",
       "      <td>Il sera proposé aux jeunes élèves d'utiliser l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Découvrez le monde fascinant des champignons</td>\n",
       "      <td>Venez découvrir le monde fascinant des champig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Comment réduire la pollution lumineuse pour am...</td>\n",
       "      <td>Comment réduire la pollution lumineuse pour am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>des recherches en cours seront présentées, ill...</td>\n",
       "      <td>Les plantes sont essentielles pour la vie sur ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Des démonstrations d’impression 3D auront lieu...</td>\n",
       "      <td>Née simultanément au Japon, en France et aux É...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vous êtes invités à réfléchir au destin partic...</td>\n",
       "      <td>Vous êtes invités à réfléchir au destin partic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  \\\n",
       "0  Comment le numérique peut nous aider à fluidif...   \n",
       "1                      Village des sciences d'Angers   \n",
       "2  Rencontre autour des nouveaux usages, de la pa...   \n",
       "3  En 2015, on fêtait la première détection d'une...   \n",
       "4  Il sera proposé aux jeunes élèves d'utiliser l...   \n",
       "5       Découvrez le monde fascinant des champignons   \n",
       "6  Comment réduire la pollution lumineuse pour am...   \n",
       "7  des recherches en cours seront présentées, ill...   \n",
       "8  Des démonstrations d’impression 3D auront lieu...   \n",
       "9  Vous êtes invités à réfléchir au destin partic...   \n",
       "\n",
       "                                             Details  \n",
       "0  Dans un monde où tous les secteurs sont transf...  \n",
       "1                                                NaN  \n",
       "2  Que recouvre aujourd’hui la notion de particip...  \n",
       "3  En 2015, on fêtait la première détection d'une...  \n",
       "4  Il sera proposé aux jeunes élèves d'utiliser l...  \n",
       "5  Venez découvrir le monde fascinant des champig...  \n",
       "6  Comment réduire la pollution lumineuse pour am...  \n",
       "7  Les plantes sont essentielles pour la vie sur ...  \n",
       "8  Née simultanément au Japon, en France et aux É...  \n",
       "9  Vous êtes invités à réfléchir au destin partic...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('events_pdloire.csv', sep=';')\n",
    "\n",
    "#population_age = df.Link.astype(np.float)\n",
    "#print(data.Description)\n",
    "#print(data.Details)\n",
    "\n",
    "df = data[['Description','Details']]\n",
    "\n",
    "df[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netoyage des données\n",
    "\n",
    "LDA a besoin que chaque phrase soit découpé sous forme d'un tableau de mots minuscules. On réfléchira par la suite sur le fait que plusieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "for item in data.Description:\n",
    "    if type(item) is str :\n",
    "        raw = item.lower()\n",
    "        token = tokenizer.tokenize(raw)\n",
    "        #print(token)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ai', 'aie', 'aient', 'aies', 'ait', 'alors', 'as', 'au', 'aucun', 'aura', 'aurai', 'auraient', 'aurais', 'aurait', 'auras', 'aurez', 'auriez', 'aurions', 'aurons', 'auront', 'aussi', 'autre', 'aux', 'avaient', 'avais', 'avait', 'avant', 'avec', 'avez', 'aviez', 'avions', 'avoir', 'avons', 'ayant', 'ayez', 'ayons', 'bon', 'car', 'ce', 'ceci', 'cela', 'ces', 'cet', 'cette', 'ceux', 'chaque', 'ci', 'comme', 'comment', 'd', 'dans', 'de', 'dedans', 'dehors', 'depuis', 'des', 'deux', 'devoir', 'devrait', 'devrez', 'devriez', 'devrions', 'devrons', 'devront', 'dois', 'doit', 'donc', 'dos', 'droite', 'du', 'dès', 'début', 'dù', 'elle', 'elles', 'en', 'encore', 'es', 'est', 'et', 'eu', 'eue', 'eues', 'eurent', 'eus', 'eusse', 'eussent', 'eusses', 'eussiez', 'eussions', 'eut', 'eux', 'eûmes', 'eût', 'eûtes', 'faire', 'fais', 'faisez', 'fait', 'faites', 'fois', 'font', 'force', 'furent', 'fus', 'fusse', 'fussent', 'fusses', 'fussiez', 'fussions', 'fut', 'fûmes', 'fût', 'fûtes', 'haut', 'hors', 'ici', 'il', 'ils', 'j', 'je', 'juste', 'l', 'la', 'le', 'les', 'leur', 'leurs', 'lui', 'là', 'm', 'ma', 'maintenant', 'mais', 'me', 'mes', 'moi', 'moins', 'mon', 'mot', 'même', 'n', 'ne', 'ni', 'nom', 'nommé', 'nommée', 'nommés', 'nos', 'notre', 'nous', 'nouveau', 'nouveaux', 'on', 'ont', 'ou', 'où', 'par', 'parce', 'parole', 'pas', 'personne', 'personnes', 'peu', 'peut', 'plupart', 'pour', 'pourquoi', 'qu', 'quand', 'que', 'quel', 'quelle', 'quelles', 'quels', 'qui', 'sa', 'sans', 'se', 'sera', 'serai', 'seraient', 'serais', 'serait', 'seras', 'serez', 'seriez', 'serions', 'serons', 'seront', 'ses', 'seulement', 'si', 'sien', 'soi', 'soient', 'sois', 'soit', 'sommes', 'son', 'sont', 'sous', 'soyez', 'soyons', 'suis', 'sujet', 'sur', 't', 'ta', 'tandis', 'te', 'tellement', 'tels', 'tes', 'toi', 'ton', 'tous', 'tout', 'trop', 'très', 'tu', 'un', 'une', 'valeur', 'voient', 'vois', 'voit', 'vont', 'vos', 'votre', 'vous', 'vu', 'y', 'à', 'ça', 'étaient', 'étais', 'était', 'étant', 'état', 'étiez', 'étions', 'été', 'étés', 'êtes', 'être', 'nan', 's', 'c', 'http', 'www', 'nantes', 'nantais', 'nantaise', 'com', 'youtube', 'facebook', 'https', 'fr', 'lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche', 'be', 'v', 'plus', 'visite', 'h', 'nan', 's', 'c', 'http', 'www', 'nantes', 'nantais', 'nantaise', 'com', 'youtube', 'facebook', 'https', 'fr', 'lundi', 'mardi', 'mercredi', 'jeudi', 'vendredi', 'samedi', 'dimanche', 'be', 'v', 'plus', 'visite', 'h']\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "#en_stop = get_stop_words('en')\n",
    "\n",
    "# create French stop words list\n",
    "fr_stop = get_stop_words('fr')\n",
    "fr_stop.append('nan')\n",
    "fr_stop.append('s')\n",
    "fr_stop.append('c')\n",
    "fr_stop.append('http')\n",
    "fr_stop.append('www')\n",
    "fr_stop.append('nantes')\n",
    "fr_stop.append('nantais')\n",
    "fr_stop.append('nantaise')\n",
    "fr_stop.append(\"com\")\n",
    "fr_stop.append(\"youtube\")\n",
    "fr_stop.append(\"facebook\")\n",
    "fr_stop.append('https')\n",
    "fr_stop.append(\"fr\")\n",
    "fr_stop.append(\"lundi\")\n",
    "fr_stop.append(\"mardi\")\n",
    "fr_stop.append(\"mercredi\")\n",
    "fr_stop.append(\"jeudi\")\n",
    "fr_stop.append(\"vendredi\")\n",
    "fr_stop.append(\"samedi\")\n",
    "fr_stop.append(\"dimanche\")\n",
    "fr_stop.append(\"be\");\n",
    "fr_stop.append(\"v\");\n",
    "fr_stop.append(\"plus\")\n",
    "fr_stop.append(\"visite\")\n",
    "fr_stop.append(\"h\")\n",
    "\n",
    "\n",
    "print(fr_stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words\n",
    "Certain parts of English speech, like conjunctions (“for”, “or”) or the word “the” are meaningless to a topic model. \n",
    "These terms are called stop words and need to be removed from our token list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On enlève les STOP WORDS des phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "tokens_stop_serie = pd.Series()\n",
    "i=0\n",
    "\n",
    "for item in data.Description:\n",
    "        raw = str(item) +' '+ str(data.Details[i])\n",
    "        token = tokenizer.tokenize(raw.lower())\n",
    "        stopped_tokens = [i for i in token if not i in fr_stop]\n",
    "        tokens_stop_serie = tokens_stop_serie.set_value(i,stopped_tokens)\n",
    "        #print(tokens_stop_serie[i])\n",
    "        i+=1\n",
    "\n",
    "        \n",
    "tokens_stop_serie[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Stemming words is another common NLP technique to reduce topically similar words to their root. \n",
    "For example, “nantaise,” “nantes,” “nantais,” all have similar meanings; stemming reduces those terms to “nant” \n",
    "This is important for topic modeling, which would otherwise view those terms as separate entities and reduce \n",
    "their importance in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "\n",
    "\n",
    "stemmer=nltk.stem.SnowballStemmer('french')\n",
    "#stemmer = FrenchStemmer()\n",
    "\n",
    "tokens_stem_serie = pd.Series()\n",
    "i = 0\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "# p_stemmer = PorterStemmer()\n",
    "\n",
    "for stop_word_token in tokens_stop_serie:\n",
    "    # stem token\n",
    "    stemmed_tokens = [stemmer.stem(i) for i in stop_word_token]\n",
    "    tokens_stem_serie = tokens_stem_serie.set_value(i,stemmed_tokens)\n",
    "    i += 1\n",
    "    \n",
    "tokens_stem_serie[0:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary(tokens_stem_serie)\n",
    "\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens_stem_serie]\n",
    "\n",
    "# The tuples are (term ID, term frequency)\n",
    "print(corpus[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the LDA model\n",
    "corpus is a document-term matrix and now we’re ready to generate an LDA model:\n",
    "\n",
    "Parameters:\n",
    "\n",
    "num_topics: required. An LDA model requires the user to determine how many topics should be generated. Our document set is small, so we’re only asking for three topics.\n",
    "id2word: required. The LdaModel class requires our previous dictionary to map ids to strings.\n",
    "passes: optional. The number of laps the model will take through corpus. The greater the number of passes, the more accurate the model will be. A lot of passes can be slow on a very large corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=5, id2word = dictionary, passes=20)\n",
    "\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.010*\"musiqu\" + 0.007*\"rock\" + 0.005*\"scen\" + 0.005*\"group\"'),\n",
       " (1, '0.014*\"jardin\" + 0.011*\"siecl\" + 0.009*\"exposit\" + 0.008*\"libr\"'),\n",
       " (2, '0.008*\"entrepris\" + 0.006*\"présent\" + 0.004*\"industr\" + 0.004*\"projet\"'),\n",
       " (3, '0.004*\"group\" + 0.004*\"présent\" + 0.004*\"univers\" + 0.003*\"the\"'),\n",
       " (4, '0.009*\"découvr\" + 0.006*\"utilis\" + 0.006*\"cré\" + 0.004*\"contenu\"')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_words_topics = ldamodel.print_topics(num_topics=10, num_words=4)\n",
    "\n",
    "best_words_topics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to Test the model !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([(0.0096838142015704311, 'musiqu'), (0.0073447281049178947, 'rock'), (0.0048136056626571335, 'scen'), (0.0047325999901999677, 'group'), (0.0046844670477632928, 'jazz')], -9.8215203104265942), ([(0.0083515792283856034, 'entrepris'), (0.0064889689255625419, 'présent'), (0.0044033117879505486, 'industr'), (0.0041041985818416529, 'projet'), (0.0037885969462532008, '2016')], -10.081467640119703), ([(0.014233067415823907, 'jardin'), (0.011332378850670091, 'siecl'), (0.0089490467476017709, 'exposit'), (0.0078167793388779529, 'libr'), (0.0070787529026439498, 'guid')], -10.246060327437798), ([(0.0092815627739202579, 'découvr'), (0.0063806654530117702, 'utilis'), (0.0056357465683065594, 'cré'), (0.0042229907569510958, 'contenu'), (0.0040316086266170099, 'contact')], -12.721008968395783), ([(0.0040909157295247762, 'group'), (0.0038462006110472112, 'présent'), (0.0038330397086012743, 'univers'), (0.0033117232114013764, 'the'), (0.003246432606810506, 'organis')], -15.139363855516146)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ldamodel.top_topics(corpus, num_words=5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'similarities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0ece62ba8c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMatrixSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"simIndex.index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdocname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"docs/the_doc.txt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'similarities' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "index = similarities.MatrixSimilarity(lda[corpus])\n",
    "index.save(\"simIndex.index\")\n",
    "\n",
    "docname = \"docs/the_doc.txt\"\n",
    "doc = open(docname, 'r').read()\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lda = lda[vec_bow]\n",
    "\n",
    "sims = index[vec_lda]\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "X = lda.datasets.load_reuters()\n",
    "vocab = lda.datasets.load_reuters_vocab()\n",
    "titles = lda.datasets.load_reuters_titles()\n",
    "X.shape\n",
    "(395, 4258)\n",
    "X.sum()\n",
    "84010\n",
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(X)  # model.fit_transform(X) is also available\n",
    "topic_word = model.topic_word_  # model.components_ also works\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]     \n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = model.doc_topic_\n",
    "for i in range(10):\n",
    "    print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
